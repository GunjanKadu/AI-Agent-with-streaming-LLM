{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d316ffc5",
   "metadata": {},
   "source": [
    "### Download Ollama from here (https://ollama.com/download/mac)\n",
    "### Download any model from the ollama (ex gpt-oss:20b)¬†-> ollama pull gpt-oss:20b¬†¬†¬†¬†¬†\n",
    "### Execute -> pip3 install langchain_ollama\n",
    "### Execute -> pip3 install langgraph\n",
    "### Execute -> pip3 install langchain\n",
    "### Follow the ollama langchain guide (https://docs.langchain.com/oss/python/integrations/chat/ollama)\n",
    "    - https://docs.langchain.com/oss/python/langgraph/quickstart#full-code-example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e251ef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a computer program, so I don't have feelings like humans do. But thank you for asking! I'm functioning properly and ready to help with any questions or tasks you may have. How about you? How's your day going so far?\", additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-12-18T09:59:59.19203Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1949466334, 'load_duration': 110700875, 'prompt_eval_count': 16, 'prompt_eval_duration': 270559417, 'eval_count': 53, 'eval_duration': 1165901701, 'logprobs': None, 'model_name': 'llama3.1:8b', 'model_provider': 'ollama'}, id='lc_run--019b30e6-e638-7fa3-bb79-e44fbf1ad3ae-0', usage_metadata={'input_tokens': 16, 'output_tokens': 53, 'total_tokens': 69})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")\n",
    "llm.invoke(\"Hello, How are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "978a32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ü§ñ TESTING AGENT WITH DEBUG OUTPUT\n",
      "============================================================\n",
      "\n",
      "üìù Question: Add 3 and 4 and 5\n",
      "------------------------------------------------------------\n",
      "  üîß add(3, 7) = 10\n",
      "\n",
      "‚úÖ Final Answer: The sum of 3, 4, and 5 is 12.\n",
      "\n",
      "============================================================\n",
      "üìù Question: Calculate (3 + 4 + 5)\n",
      "------------------------------------------------------------\n",
      "  üîß add(3, 4) = 7\n",
      "  üîß add(1, 5) = 6\n",
      "\n",
      "‚úÖ Final Answer: The result of the calculation (3 + 4 + 5) is: 12.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define tools and model\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    result = a * b\n",
    "    print(f\"  üîß multiply({a}, {b}) = {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    result = a + b\n",
    "    print(f\"  üîß add({a}, {b}) = {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    result = a / b\n",
    "    print(f\"  üîß divide({a}, {b}) = {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "model_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Step 2: Define state\n",
    "\n",
    "from langchain.messages import AnyMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    llm_calls: int\n",
    "\n",
    "# Step 3: Define model node\n",
    "from langchain.messages import SystemMessage\n",
    "\n",
    "\n",
    "def llm_call(state: dict):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            model_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"\"\"You are an expert SAP BTP consultant with deep knowledge of cloud platforms and enterprise architecture.\n",
    "                                    You can perform arithmetic calculations when needed using the provided tools.\n",
    "                                    When answering technical questions, be precise and reference official SAP terminology.\n",
    "                                 \"\"\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ],\n",
    "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
    "    }\n",
    "\n",
    "\n",
    "# Step 4: Define tool node\n",
    "\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "# Step 5: Define logic to determine whether to end\n",
    "\n",
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "# Step 6: Build agent\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END]\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "# Test with clearer question\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ü§ñ TESTING AGENT WITH DEBUG OUTPUT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Original question\n",
    "print(\"\\nüìù Question: Add 3 and 4 and 5\")\n",
    "print(\"-\" * 60)\n",
    "messages = agent.invoke({\"messages\": [HumanMessage(content=\"Add 3 and 4 and 5\")]})\n",
    "print(f\"\\n‚úÖ Final Answer: {messages['messages'][-1].content}\")\n",
    "\n",
    "# Test 2: Clearer question\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìù Question: Calculate (3 + 4 + 5)\")\n",
    "print(\"-\" * 60)\n",
    "messages = agent.invoke({\"messages\": [HumanMessage(content=\"Calculate (3 + 4 + 5)\")]})\n",
    "print(f\"\\n‚úÖ Final Answer: {messages['messages'][-1].content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17e44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
